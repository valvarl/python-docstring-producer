{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a9dfd9-359a-4c23-9a0e-870b2a08d985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a768eabf-e353-48e0-90b1-82817d955634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OTPForDocstring(L.LightningModule):\n",
    "    def __init__(self, model_name: str, learning_rate=2e-5):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.opt_model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True)\n",
    "        \n",
    "        for param in self.opt_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            if param.ndim == 1:\n",
    "                param.data = param.data.to(torch.float32)\n",
    "        self.opt_model.gradient_checkpointing_enable()\n",
    "        self.opt_model.enable_input_require_grads()\n",
    "        \n",
    "        class CastOutputToFloat(torch.nn.Sequential):\n",
    "            def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "        self.opt_model.lm_head = CastOutputToFloat(self.opt_model.lm_head)\n",
    "\n",
    "        self.lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "        self.peft_model = get_peft_model(self.opt_model, self.lora_config)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.peft_model(\n",
    "            input_ids=batch['input_ids'], \n",
    "            attention_mask=batch['attention_mask'], \n",
    "            labels=batch['labels'],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.peft_model(\n",
    "            input_ids=batch['input_ids'], \n",
    "            attention_mask=batch['attention_mask'], \n",
    "            labels=batch['labels'],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self.peft_model(\n",
    "            input_ids=batch['input_ids'], \n",
    "            attention_mask=batch['attention_mask'], \n",
    "            labels=batch['labels'],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "model = OTPForDocstring(model_name=\"facebook/opt-125m\")\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d3dcc3-6ecc-4496-a605-3b9900571924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import LearningRateFinder\n",
    "\n",
    "\n",
    "class FineTuneLearningRateFinder(LearningRateFinder):\n",
    "    def __init__(self, milestones, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.milestones = milestones\n",
    "\n",
    "    def on_fit_start(self, *args, **kwargs):\n",
    "        return\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "            self.lr_find(trainer, pl_module)\n",
    "\n",
    "# trainer = L.Trainer(max_epochs=10, devices=1, accelerator='gpu', callbacks=[FineTuneLearningRateFinder(milestones=(5, 10))])\n",
    "trainer = L.Trainer(max_epochs=10, devices=1, accelerator='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db8ed070-acae-4228-828b-101dfb12671f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CodeDataset(Dataset):\n",
    "    \n",
    "    #DATASET_NAME = 'calum/the-stack-smol-python-docstrings'\n",
    "    DATASET_NAME = '/home/valvarl/docstring-generator/the-stack-small-python-docstrings'\n",
    "    INSTRUCTION = '# code\\n```Python\\n%s\\n```\\n# docstring\\n%s'\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length=2048):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.tokenized_prompts = []\n",
    "        \n",
    "        ds = load_dataset(self.DATASET_NAME, split='train')\n",
    "        for body, docstring in zip(ds['body_without_docstring'], ds['docstring']):\n",
    "            prompt = self.INSTRUCTION % (body, docstring)\n",
    "            tokenized_prompt = self.tokenizer(prompt)\n",
    "            if len(tokenized_prompt['input_ids']) < self.max_length:\n",
    "                tokenized_prompt['input_ids'].append(self.tokenizer.eos_token_id)\n",
    "                tokenized_prompt['attention_mask'].append(1)\n",
    "                self.tokenized_prompts.append(tokenized_prompt)\n",
    "        \n",
    "        self.tokenized_prompts.sort(key=lambda x: len(x['input_ids']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_prompts)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.tokenized_prompts[idx]\n",
    "\n",
    "    \n",
    "train_data = CodeDataset(model.tokenizer, max_length=model.opt_model.config.max_position_embeddings)\n",
    "    \n",
    "train_size = int(0.8 * len(train_data))\n",
    "test_size = len(train_data) - train_size\n",
    "train_dataset, test_data = torch.utils.data.random_split(train_data, [train_size, test_size])\n",
    "\n",
    "val_size = int(0.5 * len(test_data))\n",
    "test_size = len(test_data) - val_size\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(test_data, [val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae59064c-a4f6-444b-a690-efe12f9da000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_len = max(len(tokenized_prompt['input_ids']) for tokenized_prompt in batch)\n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "    \n",
    "    for tokenized_prompt in batch:\n",
    "        input_ids = tokenized_prompt['input_ids']\n",
    "        attention_mask = tokenized_prompt['attention_mask']\n",
    "        \n",
    "        padding_length = max_len - len(input_ids)\n",
    "        padded_input_ids.append(input_ids + [tokenizer.pad_token_id] * padding_length)\n",
    "        padded_attention_mask.append(attention_mask + [0] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(padded_input_ids),\n",
    "        'attention_mask': torch.tensor(padded_attention_mask),\n",
    "        'labels': torch.tensor(padded_input_ids)\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=19, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, num_workers=19, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, num_workers=19, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037cde6c-f8de-4c95-9803-52d7b20b508e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type                 | Params\n",
      "----------------------------------------------------\n",
      "0 | opt_model  | OPTForCausalLM       | 125 M \n",
      "1 | peft_model | PeftModelForCausalLM | 125 M \n",
      "----------------------------------------------------\n",
      "589 K     Trainable params\n",
      "125 M     Non-trainable params\n",
      "125 M     Total params\n",
      "503.316   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bf281473fc498d9e43c838d3b6a0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valvarl/anaconda3/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2670b56c-49e8-42cc-8130-f1f66bfede4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92869c20838c4d4aa6c8877c2a69fa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                        | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_loss_epoch          4.7625732421875\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 4.7625732421875}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4d347e2-3f90-4a02-9953-3346018c3f1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s># code\n",
      "```Python\n",
      "def _check_u_and_t_for_simulation(m, dt, u, t, isdiscrete):\n",
      "    '\\n    \\n    '\n",
      "    if (t is None):\n",
      "        if (not isdiscrete):\n",
      "            raise ValueError('Continuous time models need an evenly spaced time sequence from which the sampling period will be obtained.')\n",
      "        else:\n",
      "            u_samples = len(u)\n",
      "            t = np.linspace(0, ((u_samples - 1) * dt), num=u_samples)\n",
      "    else:\n",
      "        t = np.asarray(t, dtype=float).squeeze()\n",
      "        if (t.ndim!= 1):\n",
      "            raise ValueError('Time array needs to be a 1D array.')\n",
      "        t_diff = np.diff(t)\n",
      "        if ((not np.allclose(t_diff, t_diff[0])) or (not (t_diff[0] > 0.0))):\n",
      "            raise ValueError('Time array should be equally spaced and increasing.')\n",
      "        if (isdiscrete and (not np.isclose(dt, t_diff[0]))):\n",
      "            raise ValueError('Time array increment {} is not equal to the model sampling period {}.'.format(t_diff[0], dt))\n",
      "    if (u.size < 1):\n",
      "        raise ValueError('The input array should at least have one point.')\n",
      "    if (len(u)!= len(t)):\n",
      "        raise ValueError('The input and time arrays should have the same length. t: {} vs. u: {}'.format(t.shape, u.shape))\n",
      "    if (u.shape[1]!= m):\n",
      "        raise ValueError(\"Number of input columns ({}) don't match the number of inputs ({}) of the given model.\".format(u.shape[1], m))\n",
      "    return t\n",
      "```\n",
      "# docstring\n",
      "Helper function to validate the input arguments for simulate_linear_system</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(test_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f49b2088-aaf6-44cd-9855-73c21e52601a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s># code\n",
      "```Python\n",
      "def load_excel(path):\n",
      "    return pd.read_excel(path)\n",
      "```\n",
      "# docstring\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('''# code\n",
    "```Python\n",
    "def load_excel(path):\n",
    "    return pd.read_excel(path)\n",
    "```\n",
    "# docstring\n",
    "''', return_tensors='pt')\n",
    "\n",
    "doc_max_length = 128\n",
    "\n",
    "generated_ids = model.peft_model.generate(\n",
    "    **inputs,\n",
    "    max_length=inputs.input_ids.shape[1] + doc_max_length,\n",
    "    do_sample=False,\n",
    "    return_dict_in_generate=True,\n",
    "    num_return_sequences=1,\n",
    "    output_scores=True,\n",
    "    pad_token_id=50256,\n",
    "    eos_token_id=50256  # <|endoftext|>\n",
    ")\n",
    "\n",
    "ret = tokenizer.decode(generated_ids.sequences[0], skip_special_tokens=False)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d68f8ee8-d547-410a-9eb3-9441fc19b2c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-33b7d2e6f98b4fd3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-33b7d2e6f98b4fd3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard \n",
    "%tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
